{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset from .pth Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pth_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a .pth file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the .pth file containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "        X (numpy array): Feature vectors (input data).\n",
    "        y (numpy array): Labels (target data).\n",
    "    \"\"\"\n",
    "    # Load the .pth file using PyTorch's torch.load method\n",
    "    data = torch.load(file_path)\n",
    "    \n",
    "    # Extract the feature vectors and convert to numpy array\n",
    "    X = data['features'].numpy()\n",
    "    \n",
    "    # Extract the labels and convert to numpy array\n",
    "    y = data['labels'].numpy()\n",
    "    \n",
    "    return X, y  # Return feature vectors and labels as numpy arrays\n",
    "\n",
    "# Paths to the processed dataset files\n",
    "train_file = \"../Data/ProcessedData/train_data.pth\"  # Path to the training data file\n",
    "test_file = \"../Data/ProcessedData/test_data.pth\"    # Path to the testing data file\n",
    "\n",
    "# Load the training data\n",
    "X_train, y_train = load_pth_data(train_file)\n",
    "# Load the testing data\n",
    "X_test, y_test = load_pth_data(test_file)\n",
    "\n",
    "# Directory to store model weights\n",
    "weights_dir = \"Weights\"\n",
    "# Create the weights directory if it doesn't already exist\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# Generate class names dynamically based on the unique classes in the training labels\n",
    "class_names = [f\"Class {i}\" for i in range(len(np.unique(y_train)))]\n",
    "\n",
    "# Print the shape of the training data and labels\n",
    "print(f\"Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}\")\n",
    "# Print the shape of the testing data and labels\n",
    "print(f\"Testing data shape: {X_test.shape}, Testing labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Custom Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node class for the Custom Decision Tree\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        \"\"\"\n",
    "        Represents a single node in the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "            feature (int): Index of the feature used for splitting at this node.\n",
    "            threshold (float): Threshold value for splitting the data.\n",
    "            left (Node): Left child node.\n",
    "            right (Node): Right child node.\n",
    "            value (int): Class label if the node is a leaf (no children).\n",
    "        \"\"\"\n",
    "        self.feature = feature        # Feature index for the split\n",
    "        self.threshold = threshold    # Threshold value for the split\n",
    "        self.left = left              # Left child node\n",
    "        self.right = right            # Right child node\n",
    "        self.value = value            # Class label if the node is a leaf\n",
    "\n",
    "\n",
    "# Custom Decision Tree class\n",
    "class CustomDecisionTree:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes an empty decision tree.\n",
    "        \"\"\"\n",
    "        self.tree = None  # Root node of the tree, initially None\n",
    "\n",
    "    def fit(self, X, y, max_depth=50):\n",
    "        \"\"\"\n",
    "        Trains the decision tree by building its structure.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy array): Feature matrix (samples x features).\n",
    "            y (numpy array): Target labels.\n",
    "            max_depth (int): Maximum depth of the tree.\n",
    "        \"\"\"\n",
    "        self.tree = self.build_tree(X, y, depth=0, max_depth=max_depth)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for a given dataset.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy array): Feature matrix (samples x features).\n",
    "\n",
    "        Returns:\n",
    "            numpy array: Predicted class labels.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"Model has not been trained or loaded.\")\n",
    "        # Predict for each sample in X\n",
    "        return np.array([self.predict_single(self.tree, x) for x in X])\n",
    "\n",
    "    def predict_single(self, node, x):\n",
    "        \"\"\"\n",
    "        Recursively traverses the tree to predict the class label for a single sample.\n",
    "\n",
    "        Parameters:\n",
    "            node (Node): Current node being evaluated.\n",
    "            x (numpy array): Feature vector for the sample.\n",
    "\n",
    "        Returns:\n",
    "            int: Predicted class label.\n",
    "        \"\"\"\n",
    "        if node.value is not None:  # Leaf node\n",
    "            return node.value\n",
    "        # Traverse left or right subtree based on the feature value and threshold\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self.predict_single(node.left, x)\n",
    "        return self.predict_single(node.right, x)\n",
    "\n",
    "    def gini_impurity(self, y):\n",
    "        \"\"\"\n",
    "        Calculates the Gini impurity for a given set of labels.\n",
    "\n",
    "        Parameters:\n",
    "            y (numpy array): Target labels.\n",
    "\n",
    "        Returns:\n",
    "            float: Gini impurity value.\n",
    "        \"\"\"\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / len(y)  # Probability of each class\n",
    "        return 1 - np.sum(probs ** 2)\n",
    "\n",
    "    def split_dataset(self, X, y, feature, threshold):\n",
    "        \"\"\"\n",
    "        Splits the dataset into two subsets based on a feature and threshold.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy array): Feature matrix.\n",
    "            y (numpy array): Target labels.\n",
    "            feature (int): Feature index used for the split.\n",
    "            threshold (float): Threshold value for the split.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (X_left, y_left, X_right, y_right) - Subsets of the data.\n",
    "        \"\"\"\n",
    "        left_idx = X[:, feature] <= threshold  # Boolean mask for the left subset\n",
    "        right_idx = X[:, feature] > threshold  # Boolean mask for the right subset\n",
    "        return X[left_idx], y[left_idx], X[right_idx], y[right_idx]\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Finds the best feature and threshold to split the dataset.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy array): Feature matrix.\n",
    "            y (numpy array): Target labels.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (best_feature, best_threshold) - Feature index and threshold for the best split.\n",
    "        \"\"\"\n",
    "        best_feature, best_threshold = None, None\n",
    "        best_impurity = float(\"inf\")  # Initialize with the highest possible impurity\n",
    "        for feature in range(X.shape[1]):  # Iterate over all features\n",
    "            thresholds = np.unique(X[:, feature])  # Unique values in the feature\n",
    "            for threshold in thresholds:  # Test each threshold\n",
    "                _, y_left, _, y_right = self.split_dataset(X, y, feature, threshold)\n",
    "                # Skip if no valid split\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "                # Calculate weighted impurity for the split\n",
    "                impurity = (\n",
    "                    len(y_left) / len(y) * self.gini_impurity(y_left) +\n",
    "                    len(y_right) / len(y) * self.gini_impurity(y_right)\n",
    "                )\n",
    "                # Update the best split if this split has lower impurity\n",
    "                if impurity < best_impurity:\n",
    "                    best_impurity = impurity\n",
    "                    best_feature, best_threshold = feature, threshold\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def build_tree(self, X, y, depth=0, max_depth=50):\n",
    "        \"\"\"\n",
    "        Recursively builds the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy array): Feature matrix.\n",
    "            y (numpy array): Target labels.\n",
    "            depth (int): Current depth of the tree.\n",
    "            max_depth (int): Maximum allowed depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "            Node: Root node of the built subtree.\n",
    "        \"\"\"\n",
    "        # Stop condition: maximum depth reached or pure node\n",
    "        if depth == max_depth or len(np.unique(y)) == 1:\n",
    "            value = np.bincount(y).argmax()  # Majority class\n",
    "            return Node(value=value)\n",
    "\n",
    "        # Find the best feature and threshold for splitting\n",
    "        feature, threshold = self.find_best_split(X, y)\n",
    "        if feature is None:  # No valid split\n",
    "            value = np.bincount(y).argmax()\n",
    "            return Node(value=value)\n",
    "\n",
    "        # Split the dataset and build child nodes recursively\n",
    "        X_left, y_left, X_right, y_right = self.split_dataset(X, y, feature, threshold)\n",
    "        left_child = self.build_tree(X_left, y_left, depth + 1, max_depth)\n",
    "        right_child = self.build_tree(X_right, y_right, depth + 1, max_depth)\n",
    "        return Node(feature, threshold, left_child, right_child)\n",
    "\n",
    "    def save_tree(self, filepath):\n",
    "        \"\"\"\n",
    "        Saves the trained tree structure to a file.\n",
    "\n",
    "        Parameters:\n",
    "            filepath (str): Path to save the tree structure.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"No tree structure to save. Train the model first.\")\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            pickle.dump(self.tree, file)  # Serialize the tree object\n",
    "            print(f\"Tree structure saved to {filepath}\")\n",
    "\n",
    "    def load_tree(self, filepath):\n",
    "        \"\"\"\n",
    "        Loads a saved tree structure from a file.\n",
    "\n",
    "        Parameters:\n",
    "            filepath (str): Path to the saved tree structure.\n",
    "        \"\"\"\n",
    "        with open(filepath, \"rb\") as file:\n",
    "            self.tree = pickle.load(file)  # Deserialize the tree object\n",
    "            print(f\"Tree structure loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics and Visualization Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, average=\"macro\"):\n",
    "    \"\"\"\n",
    "    Calculate accuracy, precision, recall, and F1-score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (array): True labels.\n",
    "        y_pred (array): Predicted labels.\n",
    "        average (str): Averaging method for precision, recall, F1-score.\n",
    "\n",
    "    Returns:\n",
    "        dict: Metrics dictionary.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average=average),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average=average),\n",
    "        \"F1-Score\": f1_score(y_true, y_pred, average=average),\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix with class labels.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (array): True labels.\n",
    "        y_pred (array): Predicted labels.\n",
    "        class_names (list): List of class names.\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Add text annotations\n",
    "    fmt = \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in depths:\n",
    "    print(f\"\\nTraining Models with Depth: {depth}\")\n",
    "    \n",
    "    # Scikit-learn Decision Tree\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=depth, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_sklearn = clf.predict(X_test)\n",
    "    model_path_sklearn = os.path.join(weights_dir, f\"Decision_Tree_Scikit_Depth{depth}.pth\")\n",
    "    with open(model_path_sklearn, \"wb\") as file:\n",
    "        pickle.dump(clf, file)\n",
    "\n",
    "    # Custom Decision Tree\n",
    "    custom_tree = CustomDecisionTree()\n",
    "    custom_tree.fit(X_train, y_train, max_depth=depth)\n",
    "    y_pred_custom = custom_tree.predict(X_test)\n",
    "    model_path_custom = os.path.join(weights_dir, f\"CustomDecisionTree_Depth{depth}.pth\")\n",
    "    custom_tree.save_tree(model_path_custom)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S In testing flow after imports, please run the cell of dataloading (2nd cell) first then the class CustomDecisionTreeh (3rd cell) \n",
    "and metrics cell ( 4th cell ) before predicition. After predictions pleas run the last cell to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and evaluating models\n",
    "for depth in depths:\n",
    "    print(f\"\\nEvaluating Models with Depth: {depth}\")\n",
    "    \n",
    "    # Load and Evaluate Scikit-learn Model\n",
    "    model_path_sklearn = os.path.join(weights_dir, f\"Decision_Tree_Scikit_Depth{depth}.pth\")\n",
    "    with open(model_path_sklearn, \"rb\") as file:\n",
    "        clf = pickle.load(file)\n",
    "    y_pred_sklearn = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    metrics_sklearn = calculate_metrics(y_test, y_pred_sklearn)\n",
    "    print(f\"Scikit-learn Metrics at Depth {depth}: {metrics_sklearn}\")\n",
    "    \n",
    "    # Calculate and print per-class metrics\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_sklearn, average=None)\n",
    "    print(f\"Scikit-learn Per-class Precision at Depth {depth}: {precision}\")\n",
    "    print(f\"Scikit-learn Per-class Recall at Depth {depth}: {recall}\")\n",
    "    print(f\"Scikit-learn Per-class F1-Score at Depth {depth}: {f1}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred_sklearn, class_names, title=f\"Scikit-learn Confusion Matrix (Depth {depth})\")\n",
    "    \n",
    "    # Load and Evaluate Custom Decision Tree\n",
    "    model_path_custom = os.path.join(weights_dir, f\"CustomDecisionTree_Depth{depth}.pth\")\n",
    "    custom_tree = CustomDecisionTree()\n",
    "    custom_tree.load_tree(model_path_custom)\n",
    "    y_pred_custom = custom_tree.predict(X_test)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    metrics_custom = calculate_metrics(y_test, y_pred_custom)\n",
    "    print(f\"Custom Decision Tree Metrics at Depth {depth}: {metrics_custom}\")\n",
    "    \n",
    "    # Calculate and print per-class metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_custom, average=None)\n",
    "    print(f\"Custom Tree Per-class Precision at Depth {depth}: {precision}\")\n",
    "    print(f\"Custom Tree Per-class Recall at Depth {depth}: {recall}\")\n",
    "    print(f\"Custom Tree Per-class F1-Score at Depth {depth}: {f1}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred_custom, class_names, title=f\"Custom Decision Tree Confusion Matrix (Depth {depth})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize Findings in a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect and display all metrics in a summary table\n",
    "metrics_summary = []  # Initialize an empty list if not already initialized\n",
    "\n",
    "for depth in depths:\n",
    "    # Load Scikit-learn model for the current depth\n",
    "    model_path_sklearn = os.path.join(weights_dir, f\"Decision_Tree_Scikit_Depth{depth}.pth\")\n",
    "    with open(model_path_sklearn, \"rb\") as file:\n",
    "        clf = pickle.load(file)\n",
    "    \n",
    "    # Make predictions and calculate metrics for Scikit-learn model\n",
    "    y_pred_sklearn = clf.predict(X_test)\n",
    "    metrics_sklearn = calculate_metrics(y_test, y_pred_sklearn)\n",
    "    metrics_sklearn[\"Depth\"] = depth\n",
    "    metrics_sklearn[\"Model\"] = \"Scikit-Learn Decision Tree\"\n",
    "    metrics_summary.append(metrics_sklearn)\n",
    "\n",
    "    # Load Custom Decision Tree model for the current depth\n",
    "    model_path_custom = os.path.join(weights_dir, f\"CustomDecisionTree_Depth{depth}.pth\")\n",
    "    custom_tree = CustomDecisionTree()\n",
    "    custom_tree.load_tree(model_path_custom)\n",
    "    \n",
    "    # Make predictions and calculate metrics for Custom Decision Tree\n",
    "    y_pred_custom = custom_tree.predict(X_test)\n",
    "    metrics_custom = calculate_metrics(y_test, y_pred_custom)\n",
    "    metrics_custom[\"Depth\"] = depth\n",
    "    metrics_custom[\"Model\"] = \"Custom Decision Tree\"\n",
    "    metrics_summary.append(metrics_custom)\n",
    "\n",
    "# Convert collected metrics to a DataFrame\n",
    "metrics_table = pd.DataFrame(metrics_summary)\n",
    "\n",
    "# Rearrange columns for better readability\n",
    "metrics_table = metrics_table[[\"Model\", \"Depth\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]]\n",
    "\n",
    "# Display the table in the notebook\n",
    "display(metrics_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
