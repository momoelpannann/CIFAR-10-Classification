{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import gzip\n",
    "import torch\n",
    "import gzip\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define The class and experiments and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a flexible MLP model with a fixed first layer and variable hidden layers\n",
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "            output_size (int): Number of output classes.\n",
    "            hidden_layers (list): List of integers specifying the size of each hidden layer after the fixed first layer.\n",
    "        \"\"\"\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # Add the fixed first layer\n",
    "        layers.append(nn.Linear(input_size, 512))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Add the variable hidden layers\n",
    "        in_features = 512\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_features = hidden_size\n",
    "\n",
    "        # Add the output layer\n",
    "        layers.append(nn.Linear(in_features, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = torch.load('../Data/ProcessedData/train_data.pth')\n",
    "train_features = train_data['features']\n",
    "train_labels = train_data['labels']\n",
    "\n",
    "test_data = torch.load('../Data/ProcessedData/test_data.pth')\n",
    "test_features = test_data['features']\n",
    "test_labels = test_data['labels']\n",
    "\n",
    "# Experiment configurations: Various layer setups after the fixed first layer\n",
    "experiment_configurations = [\n",
    "    [],                             # Remove hidden layers after the first\n",
    "    [512],                          # Inital structure\n",
    "    [512, 256],                     # One small additional hidden layer\n",
    "    [512, 1024],                    # One large  additional hidden layer\n",
    "    [512, 256, 128],                # Two small additional layers\n",
    "    [512, 2048, 1024],              # Two large additional layers\n",
    "    [512, 256, 128, 64],            # Three small additional layers \n",
    "    [512, 4096, 2048, 1024],        # Three large additional layers\n",
    "    ]      \n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the Weights directory exists\n",
    "weights_dir = \"Weights\"\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each architecture configuration for experimentation\n",
    "for idx, hidden_layers in enumerate(experiment_configurations):\n",
    "    print(f\"\\nExperiment {idx+1}: Hidden Layers - [{','.join(map(str, hidden_layers))}]\")\n",
    "    mlp_model = FlexibleMLP(input_size=50, output_size=10, hidden_layers=hidden_layers)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(mlp_model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        mlp_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = mlp_model(train_features)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, train_labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss for the final epoch\n",
    "        if epoch == num_epochs - 1:\n",
    "            print(f\"Final Epoch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Save the model weights in a compressed .gz file\n",
    "    model_name = f\"MLP_HiddenLayers_{'_'.join(map(str, hidden_layers))}_weights.pth.gz\"\n",
    "    model_save_path = os.path.join(weights_dir, model_name)\n",
    "    with gzip.open(model_save_path, \"wb\") as f:\n",
    "        torch.save(mlp_model.state_dict(), f)\n",
    "    print(f\"Compressed model weights for Experiment {idx+1} saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights and evaluate each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "P.S In testing flow after imports, please run the cell of dataloading and class FlexibleMLP (2nd cell) before Prdiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the Weights directory exists\n",
    "weights_dir = \"Weights\"\n",
    "\n",
    "# Loop through each architecture configuration for testing\n",
    "resultsfinal = []\n",
    "for idx, hidden_layers in enumerate(experiment_configurations):\n",
    "    print(f\"\\nTesting Experiment {idx+1}: Hidden Layers - [{', '.join(map(str, hidden_layers))}]\")\n",
    "    mlp_model = FlexibleMLP(input_size=50, output_size=10, hidden_layers=hidden_layers)\n",
    "    \n",
    "    # Compressed model file path\n",
    "    model_name = f\"MLP_HiddenLayers_{'_'.join(map(str, hidden_layers))}_weights.pth.gz\"\n",
    "    model_save_path = os.path.join(weights_dir, model_name)\n",
    "\n",
    "    # Load the model weights from compressed file\n",
    "    if os.path.exists(model_save_path):\n",
    "        with gzip.open(model_save_path, \"rb\") as f:\n",
    "            mlp_model.load_state_dict(torch.load(f))\n",
    "        print(f\"Model weights for Experiment {idx+1} loaded from compressed file: {model_save_path}\")\n",
    "    else:\n",
    "        print(f\"Model weights for Experiment {idx+1} not found. Skipping this configuration.\")\n",
    "        continue\n",
    "\n",
    "    # Evaluate the model\n",
    "    mlp_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = mlp_model(test_features)\n",
    "        _, test_predictions = torch.max(test_outputs, 1)\n",
    "\n",
    "    # Evaluate metrics\n",
    "    accuracy = accuracy_score(test_labels.numpy(), test_predictions.numpy())\n",
    "    precision = precision_score(test_labels.numpy(), test_predictions.numpy(), average=None)\n",
    "    recall = recall_score(test_labels.numpy(), test_predictions.numpy(), average=None)\n",
    "    f1 = f1_score(test_labels.numpy(), test_predictions.numpy(), average=None)\n",
    "\n",
    "    # Record results\n",
    "    resultsfinal.append({\n",
    "        \"Experiment\": f\"Hidden Layers: [{', '.join(map(str, hidden_layers))}]\",\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (Avg)\": np.mean(precision),\n",
    "        \"Recall (Avg)\": np.mean(recall),\n",
    "        \"F1-Score (Avg)\": np.mean(f1),\n",
    "    })\n",
    "\n",
    "    # Plot the confusion matrix for the experiment\n",
    "    cm = confusion_matrix(test_labels.numpy(), test_predictions.numpy())\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(f\"Confusion Matrix for Experiment: Hidden Layers - [{', '.join(map(str, hidden_layers))}]\")\n",
    "    plt.show()\n",
    "\n",
    "# Compile results into a DataFrame\n",
    "if resultsfinal:  # Check if resultsfinal is not empty\n",
    "    print(f\"Results collected: {len(resultsfinal)} experiments\")\n",
    "    # Print example for debugging\n",
    "    print(\"Example Result Entry:\", resultsfinal[0])\n",
    "\n",
    "    df_results = pd.DataFrame(resultsfinal)\n",
    "\n",
    "    # Ensure the correct columns exist in the DataFrame\n",
    "    columns = [\"Experiment\", \"Accuracy\", \"Precision (Avg)\", \"Recall (Avg)\", \"F1-Score (Avg)\"]\n",
    "    missing_columns = [col for col in columns if col not in df_results.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Warning: Missing columns in results: {missing_columns}\")\n",
    "    else:\n",
    "        df_results = df_results[columns]\n",
    "\n",
    "    # Format and display the results summary\n",
    "    formatted_table = tabulate(df_results, headers=\"keys\", tablefmt=\"pretty\", showindex=False)\n",
    "    print(\"Experiment Results Summary:\")\n",
    "    print(formatted_table)\n",
    "else:\n",
    "    print(\"No results found in resultsfinal. Check training or evaluation steps.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
